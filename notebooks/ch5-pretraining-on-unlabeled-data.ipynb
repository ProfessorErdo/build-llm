{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "from torch.utils.data import Dataset, DataLoader \n",
    "\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating generative text models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module): \n",
    "    def __init__(self, emb_dim): \n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "    \n",
    "    def forward(self, x): \n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift\n",
    "    \n",
    "class GELU(nn.Module): \n",
    "    def __init__(self): \n",
    "        super().__init__() \n",
    "        \n",
    "    def forward(self, x): \n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) * \n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))\n",
    "\n",
    "class FeedForward(nn.Module): \n",
    "    def __init__(self, cfg): \n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]), \n",
    "            GELU(), \n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"])\n",
    "        )\n",
    "    \n",
    "    def forward(self, x): \n",
    "        return self.layers(x)\n",
    "    \n",
    "class MultiHeadAttention(nn.Module): \n",
    "    def __init__(self, d_in, d_out, block_size, \n",
    "                 dropout, num_heads, qkv_bias=False): \n",
    "        super().__init__()\n",
    "        assert d_out % num_heads == 0, \"output dimension must be divisible by the number of heads\"\n",
    "        self.d_out = d_out \n",
    "        self.num_heads = num_heads \n",
    "        self.head_dim = d_out // num_heads \n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            'mask', \n",
    "            torch.triu(torch.ones(block_size, block_size), diagonal=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x): \n",
    "        b, num_tokens, d_in = x.shape\n",
    "        keys = self.W_key(x) \n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x) \n",
    "        \n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim) \n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        \n",
    "        keys = keys.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        \n",
    "        attn_scores = queries @ keys.transpose(2, 3)\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "        \n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "        \n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / keys.shape[-1]**0.5, dim=-1\n",
    "        )\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        context_vec = (attn_weights @ values).transpose(1, 2) \n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec)\n",
    "        return context_vec\n",
    "    \n",
    "class TransformerBlock(nn.Module): \n",
    "    def __init__(self, cfg): \n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=cfg[\"emb_dim\"], \n",
    "            d_out=cfg[\"emb_dim\"], \n",
    "            block_size=cfg[\"context_length\"], \n",
    "            num_heads=cfg[\"n_heads\"], \n",
    "            dropout=cfg[\"drop_rate\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"], \n",
    "        )\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_resid = nn.Dropout(cfg[\"drop_rate\"])\n",
    "    \n",
    "    def forward(self, x): \n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)\n",
    "        x = self.drop_resid(x)\n",
    "        x = x + shortcut \n",
    "        \n",
    "        shortcut = x \n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x) \n",
    "        x = self.drop_resid(x)\n",
    "        x = x + shortcut \n",
    "        return x\n",
    "\n",
    "class GPTModel(nn.Module): \n",
    "    def __init__(self, cfg): \n",
    "        super().__init__() \n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        \n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n",
    "        )\n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
    "        \n",
    "    def forward(self, in_idx): \n",
    "        batch_size, seq_len = in_idx.shape \n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x  = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257, \n",
    "    \"context_length\": 256, \n",
    "    \"emb_dim\": 768, \n",
    "    \"n_heads\": 12, \n",
    "    \"n_layers\": 12, \n",
    "    \"drop_rate\": 0.1, \n",
    "    \"qkv_bias\": False \n",
    "}\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(model, idx, max_new_tokens, context_size): \n",
    "    for _ in range(max_new_tokens): \n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        \n",
    "        with torch.no_grad(): \n",
    "            logits = model(idx_cond)\n",
    "        \n",
    "        logits = logits[:, -1, :]\n",
    "        probas = torch.softmax(logits, dim=-1)\n",
    "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "    \n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you rentingetic wasnÙ… refres RexMeCHicular stren\n"
     ]
    }
   ],
   "source": [
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "    return encoded_tensor \n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer): \n",
    "    flat = token_ids.squeeze(0)\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "start_context = \"Every effort moves you\" \n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\") \n",
    "\n",
    "token_ids = generate_text_simple( \n",
    "                                 model=model, \n",
    "                                 idx=text_to_token_ids(start_context, tokenizer), \n",
    "                                 max_new_tokens=10, \n",
    "                                 context_size=GPT_CONFIG_124M[\"context_length\"])\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating the text generation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor([[16833, 3626, 6100], \n",
    "                       [40, 1107, 588]])\n",
    "\n",
    "targets = torch.tensor([[3626, 6100, 345], \n",
    "                        [588, 428, 11311]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 50257])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad(): \n",
    "    logits = model(inputs)\n",
    "    \n",
    "probas = torch.softmax(logits, dim=-1)\n",
    "print(probas.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[[16657],\n",
      "         [  339],\n",
      "         [42826]],\n",
      "\n",
      "        [[49906],\n",
      "         [29669],\n",
      "         [41751]]])\n"
     ]
    }
   ],
   "source": [
    "token_ids = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "print(\"Token IDs:\\n\", token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets batch 1:  effort moves you\n",
      "Outputs batch 1:  Armed heNetflix\n"
     ]
    }
   ],
   "source": [
    "print(f\"Targets batch 1: {token_ids_to_text(targets[0], tokenizer)}\")\n",
    "print(f\"Outputs batch 1: {token_ids_to_text(token_ids[0].flatten(), tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1: tensor([7.4541e-05, 3.1061e-05, 1.1563e-05])\n",
      "Text 2: tensor([3.9836e-05, 1.6783e-05, 4.7559e-06])\n"
     ]
    }
   ],
   "source": [
    "text_idx = 0\n",
    "target_probas_1 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 1:\", target_probas_1)\n",
    "\n",
    "text_idx = 1\n",
    "target_probas_2 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 2:\", target_probas_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ -9.5042, -10.3796, -11.3677, -10.1308, -10.9951, -12.2561])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_probas = torch.log(torch.cat((target_probas_1, target_probas_2)))\n",
    "log_probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-10.7722)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_log_probas = torch.mean(log_probas)\n",
    "avg_log_probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.7722)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_avg_log_probas = avg_log_probas * -1\n",
    "neg_avg_log_probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([2, 3, 50257])\n",
      "Targets shape: torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "print(\"Logits shape:\", logits.shape)\n",
    "print(\"Targets shape:\", targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened logits: torch.Size([6, 50257])\n",
      "flattened targets: torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "logits_flat = logits.flatten(0, 1)\n",
    "targets_flat = targets.flatten()\n",
    "print(\"Flattened logits:\", logits_flat.shape)\n",
    "print(\"flattened targets:\", targets_flat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.7722)\n"
     ]
    }
   ],
   "source": [
    "loss = F.cross_entropy(logits_flat, targets_flat)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(47678.8633)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexity = torch.exp(loss)\n",
    "perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"../data/the-verdict.txt\" \n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file: \n",
    "    text_data = file.read() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters: 20479\n",
      "Tokens: 5145\n"
     ]
    }
   ],
   "source": [
    "total_characters = len(text_data) \n",
    "total_tokens = len(tokenizer.encode(text_data))\n",
    "\n",
    "print(\"Characters:\", total_characters)\n",
    "print(\"Tokens:\", total_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.90 \n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTDatasetV1(Dataset): \n",
    "    def __init__(self, txt, tokenizer, max_length, stride): \n",
    "        self.tokenizer = tokenizer \n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "        \n",
    "        token_ids = tokenizer.encode(txt)\n",
    "        \n",
    "        for i in range(0, len(token_ids) - max_length, stride): \n",
    "            input_chunk = token_ids[i: i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "    \n",
    "    def __len__(self): \n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx): \n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "    \n",
    "def create_dataloader_v1(txt, batch_size=4, \n",
    "                         max_length=256, stride=128, shuffle=True, drop_last=True): \n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride) \n",
    "    dataloader = DataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last\n",
    "    )\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data, \n",
    "    batch_size=2, \n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"], \n",
    "    stride=GPT_CONFIG_124M[\"context_length\"], \n",
    "    drop_last=True, \n",
    "    shuffle=True\n",
    ")\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data, \n",
    "    batch_size=2, \n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"], \n",
    "    stride=GPT_CONFIG_124M[\"context_length\"], \n",
    "    drop_last=False, \n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "\n",
      "Validation loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loader:\")\n",
    "for x, y in train_loader: \n",
    "    print(x.shape, y.shape) \n",
    "\n",
    "print(\"\\nValidation loader:\") \n",
    "for x, y in val_loader: \n",
    "    print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device): \n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    model = model.to(device) # move model to mps\n",
    "    logits = model(input_batch) \n",
    "    loss = F.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
    "    return loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_loader(data_loader, model, device, num_batches=None): \n",
    "    total_loss = 0 \n",
    "    if num_batches is None: \n",
    "        num_batches = len(data_loader)\n",
    "    else: \n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader): \n",
    "        if i < num_batches: \n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device) \n",
    "            total_loss += loss.item() \n",
    "        else: \n",
    "            break \n",
    "    return total_loss / num_batches "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() \n",
    "                      else \"mps\" if torch.backends.mps.is_available() \n",
    "                      else \"cpu\")\n",
    "\n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available()\n",
    "#                      else \"cpu\")\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 10.98758316040039\n",
      "Validation loss: 10.98110580444336\n"
     ]
    }
   ],
   "source": [
    "train_loss = calc_loss_loader(train_loader, model, device)\n",
    "val_loss = calc_loss_loader(val_loader, model, device) \n",
    "print(\"Training loss:\", train_loss) \n",
    "print(\"Validation loss:\", val_loss) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, train_loader, val_laoder, device, eval_iter): \n",
    "    model.eval() \n",
    "    with torch.no_grad(): \n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train() \n",
    "    return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_print_sample(model, tokenizer, device, start_context): \n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(\n",
    "            model=model, idx=encoded, \n",
    "            max_new_tokens=50, context_size=context_size)\n",
    "        decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "        print(decoded_text.replace(\"\\n\", \" \")) # compact print\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs, eval_freq, eval_iter, start_context): \n",
    "    train_losses, val_losses, track_tokens_seen = [], [], [] \n",
    "    tokens_seen, global_steps = 0, -1 \n",
    "    \n",
    "    for epoch in range(num_epochs): \n",
    "        model.train() \n",
    "        for input_batch, target_batch in train_loader: \n",
    "            optimizer.zero_grad() \n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward() \n",
    "            optimizer.step() \n",
    "            tokens_seen += input_batch.numel() \n",
    "            global_steps += 1 \n",
    "            \n",
    "            if global_steps % eval_freq == 0: \n",
    "                train_loss, val_loss = evaluate_model(model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss) \n",
    "                val_losses.append(val_loss) \n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Ep {epoch+1} (Step {global_steps:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "                \n",
    "        generate_and_print_sample(\n",
    "            model, train_loader.dataset.tokenizer, device, start_context\n",
    "        )\n",
    "    return train_losses, val_losses, track_tokens_seen         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 10.058, Val loss 9.924\n",
      "Ep 1 (Step 000005): Train loss 8.153, Val loss 8.332\n",
      "Every effort moves you,,,,,,,,,,,,.                                     \n",
      "Ep 2 (Step 000010): Train loss 6.551, Val loss 7.042\n",
      "Ep 2 (Step 000015): Train loss 5.921, Val loss 6.596\n",
      "Every effort moves you, and,, and, and,,,,, and, and,,,,,,,,,,, and,, the,, the, and,, and,,, the, and,,,,,,\n",
      "Ep 3 (Step 000020): Train loss 5.813, Val loss 6.508\n",
      "Ep 3 (Step 000025): Train loss 5.369, Val loss 6.378\n",
      "Every effort moves you, and to the of the of the picture. Gis.                                     \n",
      "Ep 4 (Step 000030): Train loss 4.460, Val loss 6.263\n",
      "Ep 4 (Step 000035): Train loss 4.943, Val loss 6.285\n",
      "Every effort moves you of the \"I the picture.                    \"I\"I the picture\"I had the picture\"I the picture and I had been the picture of\n",
      "Ep 5 (Step 000040): Train loss 3.918, Val loss 6.130\n",
      "Every effort moves you know he had been his pictures, and I felt it's by his last word.                   \"Oh, and he had been the end, and he had been\n",
      "Ep 6 (Step 000045): Train loss 3.855, Val loss 6.183\n",
      "Ep 6 (Step 000050): Train loss 2.721, Val loss 6.123\n",
      "Every effort moves you know it was his pictures--I glanced after him, I had the last word.        \"Oh, and I was his pictures--I looked.   \"I looked. \"I looked. \n",
      "Ep 7 (Step 000055): Train loss 2.813, Val loss 6.150\n",
      "Ep 7 (Step 000060): Train loss 1.894, Val loss 6.133\n",
      "Every effort moves you know the picture to me--I glanced after him, and Mrs.  \"I was no great, the fact, the fact that, the moment--as Jack himself, as his pictures--as of the picture--because he was a little\n",
      "Ep 8 (Step 000065): Train loss 1.787, Val loss 6.186\n",
      "Ep 8 (Step 000070): Train loss 1.234, Val loss 6.230\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the fact with a little: \"Yes--and by me to me to have to see a smile behind his close grayish beard--as if he had the donkey. \"There were days when I\n",
      "Ep 9 (Step 000075): Train loss 1.093, Val loss 6.251\n",
      "Ep 9 (Step 000080): Train loss 0.708, Val loss 6.278\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the fact with a laugh: \"Yes--and by me!\"  He laughed again, and threw back the window-curtains, I saw that, and down the room, and now\n",
      "Ep 10 (Step 000085): Train loss 0.502, Val loss 6.373\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123) \n",
    "\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(device) \n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
    "num_epochs = 10\n",
    "train_losses, val_losses, token_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, device, \n",
    "    num_epochs=num_epochs, eval_freq=5, eval_iter=1, \n",
    "    start_context=\"Every effort moves you\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses): \n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3)) \n",
    "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
    "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\") \n",
    "    ax2 = ax1.twiny() \n",
    "    ax2.plot(tokens_seen, train_losses, alpha=0) \n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "    fig.tight_layout() \n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, token_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DECODING STRATEGIES TO CONTROL RANDOMNESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(\"cpu\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you?\"\n",
      "\n",
      "\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "token_ids = generate_text_simple(\n",
    "    model=model, \n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer), \n",
    "    max_new_tokens=25, \n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Temperature scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {\n",
    "    \"closer\": 0, \n",
    "    \"every\": 1, \n",
    "    \"effort\": 2, \n",
    "    \"forward\": 3, \n",
    "    \"inches\": 4, \n",
    "    \"moves\": 5, \n",
    "    \"pizza\": 6,  \n",
    "    \"toward\": 7, \n",
    "    \"you\": 8\n",
    "}\n",
    "inverse_vocab = {v: k for k, v in vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_token_logits = torch.tensor(\n",
    "    [4.51, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward\n"
     ]
    }
   ],
   "source": [
    "probas = torch.softmax(next_token_logits, dim=0)\n",
    "next_token_id = torch.argmax(probas).item() \n",
    "print(inverse_vocab[next_token_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123) \n",
    "next_token_id = torch.multinomial(probas, num_samples=1).item() \n",
    "print(inverse_vocab[next_token_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73 x closer\n",
      "0 x every\n",
      "0 x effort\n",
      "582 x forward\n",
      "2 x inches\n",
      "0 x moves\n",
      "0 x pizza\n",
      "343 x toward\n"
     ]
    }
   ],
   "source": [
    "def print_sampled_tokens(probas): \n",
    "    torch.manual_seed(123) \n",
    "    sample = [torch.multinomial(probas, num_samples=1).item() for i in range(1000)]\n",
    "    sampled_ids = torch.bincount(torch.tensor(sample))\n",
    "    for i, freq in enumerate(sampled_ids): \n",
    "        print(f\"{freq} x {inverse_vocab[i]}\")\n",
    "\n",
    "print_sampled_tokens(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_with_temperature(logits, temperature): \n",
    "    scaled_logits = logits / temperature \n",
    "    return torch.softmax(scaled_logits, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "temperatures = [1, 0.1, 5] \n",
    "scaled_probas = [softmax_with_temperature(next_token_logits, T) for T in temperatures]\n",
    "x = torch.arange(len(vocab))\n",
    "bar_width = 0.15 \n",
    "fig, ax = plt.subplots(figsize=(5, 3))\n",
    "for i, T in enumerate(temperatures): \n",
    "    rects = ax.bar(x + i * bar_width, scaled_probas[i], \n",
    "                   bar_width, label=f'Temperature = {T}')\n",
    "ax.set_ylabel('Probability')\n",
    "ax.set_xticks(x) \n",
    "ax.set_xticklabels(vocab.keys(), rotation=90)\n",
    "ax.legend() \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73 x closer\n",
      "0 x every\n",
      "0 x effort\n",
      "582 x forward\n",
      "2 x inches\n",
      "0 x moves\n",
      "0 x pizza\n",
      "343 x toward\n"
     ]
    }
   ],
   "source": [
    "def print_sampled_tokens(logits, temperature): \n",
    "    torch.manual_seed(123) \n",
    "    scaled_probas = softmax_with_temperature(logits, temperature=temperature)\n",
    "    sample = [torch.multinomial(scaled_probas, num_samples=1).item() for i in range(1000)]\n",
    "    sampled_ids = torch.bincount(torch.tensor(sample))\n",
    "    for i, freq in enumerate(sampled_ids): \n",
    "        print(f\"{freq} x {inverse_vocab[i]}\")\n",
    "\n",
    "print_sampled_tokens(next_token_logits, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 x closer\n",
      "0 x every\n",
      "0 x effort\n",
      "985 x forward\n",
      "0 x inches\n",
      "0 x moves\n",
      "0 x pizza\n",
      "15 x toward\n"
     ]
    }
   ],
   "source": [
    "print_sampled_tokens(next_token_logits, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "165 x closer\n",
      "75 x every\n",
      "42 x effort\n",
      "239 x forward\n",
      "71 x inches\n",
      "46 x moves\n",
      "32 x pizza\n",
      "227 x toward\n",
      "103 x you\n"
     ]
    }
   ],
   "source": [
    "print_sampled_tokens(next_token_logits, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top logits: tensor([6.7500, 6.2800, 4.5100])\n",
      "Top positions: tensor([3, 7, 0])\n"
     ]
    }
   ],
   "source": [
    "top_k = 3 \n",
    "top_logits, top_pos = torch.topk(next_token_logits, top_k) \n",
    "print(\"Top logits:\", top_logits)\n",
    "print(\"Top positions:\", top_pos) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.5100,   -inf,   -inf, 6.7500,   -inf,   -inf,   -inf, 6.2800,   -inf])\n"
     ]
    }
   ],
   "source": [
    "new_logits = torch.where(\n",
    "    condition=next_token_logits < top_logits[-1], \n",
    "    input=torch.tensor(float('-inf')), \n",
    "    other=next_token_logits \n",
    ")\n",
    "print(new_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0615, 0.0000, 0.0000, 0.5775, 0.0000, 0.0000, 0.0000, 0.3610, 0.0000])\n"
     ]
    }
   ],
   "source": [
    "topk_probas = torch.softmax(new_logits, dim=0)\n",
    "print(topk_probas) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, idx, max_new_tokens, context_size, top_k, temperature): \n",
    "    for _ in range(max_new_tokens): \n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad(): \n",
    "            logits = model(idx_cond) \n",
    "        logits = logits[:, -1, :]\n",
    "        if top_k is not None: \n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val = top_logits[:, -1] \n",
    "            logits = torch.where(\n",
    "                logits < min_val, \n",
    "                torch.tensor(float('-inf')).to(logits.device), \n",
    "                logits\n",
    "            )\n",
    "        if temperature > 0.0: \n",
    "            logits = logits /temperature \n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "        else:\n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you stand,\" she down.\" For Mrs. Gisburn! The women had\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "token_ids = generate(\n",
    "    model=model, \n",
    "    idx=text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens=15, \n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "    top_k=25, \n",
    "    temperature=1.4\n",
    ")\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"../models/model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.load_state_dict(torch.load(\"../models/model.pth\"))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    \"model_state_dict\": model.state_dict(), \n",
    "    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "    }, \n",
    "    \"model_and_optimizer.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load(\"model_and_optimizer.pth\")\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=0.1)\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
