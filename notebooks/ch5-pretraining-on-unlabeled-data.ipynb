{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset, DataLoader \n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtiktoken\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GPTModel\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'model'"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "from torch.utils.data import Dataset, DataLoader \n",
    "\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating generative text models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module): \n",
    "    def __init__(self, emb_dim): \n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "    \n",
    "    def forward(self, x): \n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift\n",
    "    \n",
    "class GELU(nn.Module): \n",
    "    def __init__(self): \n",
    "        super().__init__() \n",
    "        \n",
    "    def forward(self, x): \n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) * \n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))\n",
    "\n",
    "class FeedForward(nn.Module): \n",
    "    def __init__(self, cfg): \n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]), \n",
    "            GELU(), \n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"])\n",
    "        )\n",
    "    \n",
    "    def forward(self, x): \n",
    "        return self.layers(x)\n",
    "    \n",
    "class MultiHeadAttention(nn.Module): \n",
    "    def __init__(self, d_in, d_out, block_size, \n",
    "                 dropout, num_heads, qkv_bias=False): \n",
    "        super().__init__()\n",
    "        assert d_out % num_heads == 0, \"output dimension must be divisible by the number of heads\"\n",
    "        self.d_out = d_out \n",
    "        self.num_heads = num_heads \n",
    "        self.head_dim = d_out // num_heads \n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            'mask', \n",
    "            torch.triu(torch.ones(block_size, block_size), diagonal=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x): \n",
    "        b, num_tokens, d_in = x.shape\n",
    "        keys = self.W_key(x) \n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x) \n",
    "        \n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim) \n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        \n",
    "        keys = keys.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        \n",
    "        attn_scores = queries @ keys.transpose(2, 3)\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "        \n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "        \n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / keys.shape[-1]**0.5, dim=-1\n",
    "        )\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        context_vec = (attn_weights @ values).transpose(1, 2) \n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec)\n",
    "        return context_vec\n",
    "    \n",
    "class TransformerBlock(nn.Module): \n",
    "    def __init__(self, cfg): \n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=cfg[\"emb_dim\"], \n",
    "            d_out=cfg[\"emb_dim\"], \n",
    "            block_size=cfg[\"context_length\"], \n",
    "            num_heads=cfg[\"n_heads\"], \n",
    "            dropout=cfg[\"drop_rate\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"], \n",
    "        )\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_resid = nn.Dropout(cfg[\"drop_rate\"])\n",
    "    \n",
    "    def forward(self, x): \n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)\n",
    "        x = self.drop_resid(x)\n",
    "        x = x + shortcut \n",
    "        \n",
    "        shortcut = x \n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x) \n",
    "        x = self.drop_resid(x)\n",
    "        x = x + shortcut \n",
    "        return x\n",
    "\n",
    "class GPTModel(nn.Module): \n",
    "    def __init__(self, cfg): \n",
    "        super().__init__() \n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        \n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n",
    "        )\n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
    "        \n",
    "    def forward(self, in_idx): \n",
    "        batch_size, seq_len = in_idx.shape \n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x  = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257, \n",
    "    \"context_length\": 256, \n",
    "    \"emb_dim\": 768, \n",
    "    \"n_heads\": 12, \n",
    "    \"n_layers\": 12, \n",
    "    \"drop_rate\": 0.1, \n",
    "    \"qkv_bias\": False \n",
    "}\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(model, idx, max_new_tokens, context_size): \n",
    "    for _ in range(max_new_tokens): \n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        \n",
    "        with torch.no_grad(): \n",
    "            logits = model(idx_cond)\n",
    "        \n",
    "        logits = logits[:, -1, :]\n",
    "        probas = torch.softmax(logits, dim=-1)\n",
    "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "    \n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you rentingetic wasnم refres RexMeCHicular stren\n"
     ]
    }
   ],
   "source": [
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "    return encoded_tensor \n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer): \n",
    "    flat = token_ids.squeeze(0)\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "start_context = \"Every effort moves you\" \n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\") \n",
    "\n",
    "token_ids = generate_text_simple( \n",
    "                                 model=model, \n",
    "                                 idx=text_to_token_ids(start_context, tokenizer), \n",
    "                                 max_new_tokens=10, \n",
    "                                 context_size=GPT_CONFIG_124M[\"context_length\"])\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating the text generation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor([[16833, 3626, 6100], \n",
    "                       [40, 1107, 588]])\n",
    "\n",
    "targets = torch.tensor([[3626, 6100, 345], \n",
    "                        [588, 428, 11311]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 50257])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad(): \n",
    "    logits = model(inputs)\n",
    "    \n",
    "probas = torch.softmax(logits, dim=-1)\n",
    "print(probas.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[[16657],\n",
      "         [  339],\n",
      "         [42826]],\n",
      "\n",
      "        [[49906],\n",
      "         [29669],\n",
      "         [41751]]])\n"
     ]
    }
   ],
   "source": [
    "token_ids = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "print(\"Token IDs:\\n\", token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets batch 1:  effort moves you\n",
      "Outputs batch 1:  Armed heNetflix\n"
     ]
    }
   ],
   "source": [
    "print(f\"Targets batch 1: {token_ids_to_text(targets[0], tokenizer)}\")\n",
    "print(f\"Outputs batch 1: {token_ids_to_text(token_ids[0].flatten(), tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1: tensor([7.4541e-05, 3.1061e-05, 1.1563e-05])\n",
      "Text 2: tensor([3.9836e-05, 1.6783e-05, 4.7559e-06])\n"
     ]
    }
   ],
   "source": [
    "text_idx = 0\n",
    "target_probas_1 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 1:\", target_probas_1)\n",
    "\n",
    "text_idx = 1\n",
    "target_probas_2 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 2:\", target_probas_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ -9.5042, -10.3796, -11.3677, -10.1308, -10.9951, -12.2561])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_probas = torch.log(torch.cat((target_probas_1, target_probas_2)))\n",
    "log_probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-10.7722)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_log_probas = torch.mean(log_probas)\n",
    "avg_log_probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.7722)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_avg_log_probas = avg_log_probas * -1\n",
    "neg_avg_log_probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([2, 3, 50257])\n",
      "Targets shape: torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "print(\"Logits shape:\", logits.shape)\n",
    "print(\"Targets shape:\", targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened logits: torch.Size([6, 50257])\n",
      "flattened targets: torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "logits_flat = logits.flatten(0, 1)\n",
    "targets_flat = targets.flatten()\n",
    "print(\"Flattened logits:\", logits_flat.shape)\n",
    "print(\"flattened targets:\", targets_flat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.7722)\n"
     ]
    }
   ],
   "source": [
    "loss = F.cross_entropy(logits_flat, targets_flat)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(47678.8633)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexity = torch.exp(loss)\n",
    "perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"../data/the-verdict.txt\" \n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file: \n",
    "    text_data = file.read() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters: 20479\n",
      "Tokens: 5145\n"
     ]
    }
   ],
   "source": [
    "total_characters = len(text_data) \n",
    "total_tokens = len(tokenizer.encode(text_data))\n",
    "\n",
    "print(\"Characters:\", total_characters)\n",
    "print(\"Tokens:\", total_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.90 \n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTDatasetV1(Dataset): \n",
    "    def __init__(self, txt, tokenizer, max_length, stride): \n",
    "        self.tokenizer = tokenizer \n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "        \n",
    "        token_ids = tokenizer.encode(txt)\n",
    "        \n",
    "        for i in range(0, len(token_ids) - max_length, stride): \n",
    "            input_chunk = token_ids[i: i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "    \n",
    "    def __len__(self): \n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx): \n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "    \n",
    "def create_dataloader_v1(txt, batch_size=4, \n",
    "                         max_length=256, stride=128, shuffle=True, drop_last=True): \n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride) \n",
    "    dataloader = DataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last\n",
    "    )\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data, \n",
    "    batch_size=2, \n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"], \n",
    "    stride=GPT_CONFIG_124M[\"context_length\"], \n",
    "    drop_last=True, \n",
    "    shuffle=True\n",
    ")\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data, \n",
    "    batch_size=2, \n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"], \n",
    "    stride=GPT_CONFIG_124M[\"context_length\"], \n",
    "    drop_last=False, \n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "\n",
      "Validation loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loader:\")\n",
    "for x, y in train_loader: \n",
    "    print(x.shape, y.shape) \n",
    "\n",
    "print(\"\\nValidation loader:\") \n",
    "for x, y in val_loader: \n",
    "    print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device): \n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    model = model.to(device) # move model to mps\n",
    "    logits = model(input_batch) \n",
    "    loss = F.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
    "    return loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_loader(data_loader, model, device, num_batches=None): \n",
    "    total_loss = 0 \n",
    "    if num_batches is None: \n",
    "        num_batches = len(data_loader)\n",
    "    else: \n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader): \n",
    "        if i < num_batches: \n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device) \n",
    "            total_loss += loss.item() \n",
    "        else: \n",
    "            break \n",
    "    return total_loss / num_batches "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() \n",
    "                      else \"mps\" if torch.backends.mps.is_available() \n",
    "                      else \"cpu\")\n",
    "\n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available()\n",
    "#                      else \"cpu\")\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 10.987583266364204\n",
      "Validation loss: 10.98110580444336\n"
     ]
    }
   ],
   "source": [
    "train_loss = calc_loss_loader(train_loader, model, device)\n",
    "val_loss = calc_loss_loader(val_loader, model, device) \n",
    "print(\"Training loss:\", train_loss) \n",
    "print(\"Validation loss:\", val_loss) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, train_loader, val_laoder, device, eval_iter): \n",
    "    model.eval() \n",
    "    with torch.no_grad(): \n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train() \n",
    "    return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_print_sample(model, tokenizer, device, start_context): \n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(\n",
    "            model=model, idx=encoded, \n",
    "            max_new_tokens=50, context_size=context_size)\n",
    "        decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "        print(decoded_text.replace(\"\\n\", \" \")) # compact print\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs, eval_freq, eval_iter, start_context): \n",
    "    train_losses, val_losses, track_tokens_seen = [], [], [] \n",
    "    tokens_seen, global_steps = 0, -1 \n",
    "    \n",
    "    for epoch in range(num_epochs): \n",
    "        model.train() \n",
    "        for input_batch, target_batch in train_loader: \n",
    "            optimizer.zero_grad() \n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward() \n",
    "            optimizer.step() \n",
    "            tokens_seen += input_batch.numel() \n",
    "            global_steps += 1 \n",
    "            \n",
    "            if global_steps % eval_freq == 0: \n",
    "                train_loss, val_loss = evaluate_model(model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss) \n",
    "                val_losses.append(val_loss) \n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Ep {epoch+1} (Step {global_steps:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "                \n",
    "        generate_and_print_sample(\n",
    "            model, train_loader.dataset.tokenizer, device, start_context\n",
    "        )\n",
    "    return train_losses, val_losses, track_tokens_seen         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/pytorch/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 10.058, Val loss 9.924\n",
      "Ep 1 (Step 000005): Train loss 8.334, Val loss 8.558\n",
      "Every effort moves you,,,,,,,,,,,,.                                     \n",
      "Ep 2 (Step 000010): Train loss 6.633, Val loss 7.144\n",
      "Ep 2 (Step 000015): Train loss 6.329, Val loss 6.826\n",
      "Every effort moves you, the,,,,,,,,,,,,,,,,,,,,,,,,,,, the,,,,,,,,,,,,,,,,,,,,\n",
      "Ep 3 (Step 000020): Train loss 6.265, Val loss 6.678\n",
      "Ep 3 (Step 000025): Train loss 6.001, Val loss 6.677\n",
      "Every effort moves you                                                  \n",
      "Ep 4 (Step 000030): Train loss 5.999, Val loss 6.703\n",
      "Ep 4 (Step 000035): Train loss 6.122, Val loss 6.708\n",
      "Every effort moves you                                                  \n",
      "Ep 5 (Step 000040): Train loss 5.961, Val loss 6.717\n",
      "Every effort moves you,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n",
      "Ep 6 (Step 000045): Train loss 6.064, Val loss 6.738\n",
      "Ep 6 (Step 000050): Train loss 5.939, Val loss 6.762\n",
      "Every effort moves you,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n",
      "Ep 7 (Step 000055): Train loss 6.079, Val loss 6.790\n",
      "Ep 7 (Step 000060): Train loss 5.859, Val loss 6.805\n",
      "Every effort moves you                                                  \n",
      "Ep 8 (Step 000065): Train loss 5.901, Val loss 6.788\n",
      "Ep 8 (Step 000070): Train loss 6.009, Val loss 6.791\n",
      "Every effort moves you,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n",
      "Ep 9 (Step 000075): Train loss 6.105, Val loss 6.798\n",
      "Ep 9 (Step 000080): Train loss 6.121, Val loss 6.821\n",
      "Every effort moves you,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n",
      "Ep 10 (Step 000085): Train loss 5.847, Val loss 6.838\n",
      "Every effort moves you                                                  \n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123) \n",
    "\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(device) \n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
    "num_epochs = 10\n",
    "train_losses, val_losses, token_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, device, \n",
    "    num_epochs=num_epochs, eval_freq=5, eval_iter=1, \n",
    "    start_context=\"Every effort moves you\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses): \n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3)) \n",
    "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
    "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\") \n",
    "    ax2 = ax1.twiny() \n",
    "    ax2.plot(tokens_seen, train_losses, alpha=0) \n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "    fig.tight_layout() \n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, token_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DECODING STRATEGIES TO CONTROL RANDOMNESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(\"cpu\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "token_ids = generate_text_simple(\n",
    "    model=model, \n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer), \n",
    "    max_new_tokens=25, \n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Temperature scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {\n",
    "    \"closer\": 0, \n",
    "    \"every\": 1, \n",
    "    \"effort\": 2, \n",
    "    \"forward\": 3, \n",
    "    \"inches\": 4, \n",
    "    \"moves\": 5, \n",
    "    \"pizza\": 6,  \n",
    "    \"toward\": 7, \n",
    "    \"you\": 8\n",
    "}\n",
    "inverse_vocab = {v: k for k, v in vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_token_logits = torch.tensor(\n",
    "    [4.51, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward\n"
     ]
    }
   ],
   "source": [
    "probas = torch.softmax(next_token_logits, dim=0)\n",
    "next_token_id = torch.argmax(probas).item() \n",
    "print(inverse_vocab[next_token_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123) \n",
    "next_token_id = torch.multinomial(probas, num_samples=1).item() \n",
    "print(inverse_vocab[next_token_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73 x closer\n",
      "0 x every\n",
      "0 x effort\n",
      "582 x forward\n",
      "2 x inches\n",
      "0 x moves\n",
      "0 x pizza\n",
      "343 x toward\n"
     ]
    }
   ],
   "source": [
    "def print_sampled_tokens(probas): \n",
    "    torch.manual_seed(123) \n",
    "    sample = [torch.multinomial(probas, num_samples=1).item() for i in range(1000)]\n",
    "    sampled_ids = torch.bincount(torch.tensor(sample))\n",
    "    for i, freq in enumerate(sampled_ids): \n",
    "        print(f\"{freq} x {inverse_vocab[i]}\")\n",
    "\n",
    "print_sampled_tokens(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_with_temperature(logits, temperature): \n",
    "    scaled_logits = logits / temperature \n",
    "    return torch.softmax(scaled_logits, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "temperatures = [1, 0.1, 5] \n",
    "scaled_probas = [softmax_with_temperature(next_token_logits, T) for T in temperatures]\n",
    "x = torch.arange(len(vocab))\n",
    "bar_width = 0.15 \n",
    "fig, ax = plt.subplots(figsize=(5, 3))\n",
    "for i, T in enumerate(temperatures): \n",
    "    rects = ax.bar(x + i * bar_width, scaled_probas[i], \n",
    "                   bar_width, label=f'Temperature = {T}')\n",
    "ax.set_ylabel('Probability')\n",
    "ax.set_xticks(x) \n",
    "ax.set_xticklabels(vocab.keys(), rotation=90)\n",
    "ax.legend() \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73 x closer\n",
      "0 x every\n",
      "0 x effort\n",
      "582 x forward\n",
      "2 x inches\n",
      "0 x moves\n",
      "0 x pizza\n",
      "343 x toward\n"
     ]
    }
   ],
   "source": [
    "def print_sampled_tokens(logits, temperature): \n",
    "    torch.manual_seed(123) \n",
    "    scaled_probas = softmax_with_temperature(logits, temperature=temperature)\n",
    "    sample = [torch.multinomial(scaled_probas, num_samples=1).item() for i in range(1000)]\n",
    "    sampled_ids = torch.bincount(torch.tensor(sample))\n",
    "    for i, freq in enumerate(sampled_ids): \n",
    "        print(f\"{freq} x {inverse_vocab[i]}\")\n",
    "\n",
    "print_sampled_tokens(next_token_logits, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 x closer\n",
      "0 x every\n",
      "0 x effort\n",
      "985 x forward\n",
      "0 x inches\n",
      "0 x moves\n",
      "0 x pizza\n",
      "15 x toward\n"
     ]
    }
   ],
   "source": [
    "print_sampled_tokens(next_token_logits, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "165 x closer\n",
      "75 x every\n",
      "42 x effort\n",
      "239 x forward\n",
      "71 x inches\n",
      "46 x moves\n",
      "32 x pizza\n",
      "227 x toward\n",
      "103 x you\n"
     ]
    }
   ],
   "source": [
    "print_sampled_tokens(next_token_logits, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top logits: tensor([6.7500, 6.2800, 4.5100])\n",
      "Top positions: tensor([3, 7, 0])\n"
     ]
    }
   ],
   "source": [
    "top_k = 3 \n",
    "top_logits, top_pos = torch.topk(next_token_logits, top_k) \n",
    "print(\"Top logits:\", top_logits)\n",
    "print(\"Top positions:\", top_pos) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.5100,   -inf,   -inf, 6.7500,   -inf,   -inf,   -inf, 6.2800,   -inf])\n"
     ]
    }
   ],
   "source": [
    "new_logits = torch.where(\n",
    "    condition=next_token_logits < top_logits[-1], \n",
    "    input=torch.tensor(float('-inf')), \n",
    "    other=next_token_logits \n",
    ")\n",
    "print(new_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0615, 0.0000, 0.0000, 0.5775, 0.0000, 0.0000, 0.0000, 0.3610, 0.0000])\n"
     ]
    }
   ],
   "source": [
    "topk_probas = torch.softmax(new_logits, dim=0)\n",
    "print(topk_probas) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, idx, max_new_tokens, context_size, top_k, temperature): \n",
    "    for _ in range(max_new_tokens): \n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad(): \n",
    "            logits = model(idx_cond) \n",
    "        logits = logits[:, -1, :]\n",
    "        if top_k is not None: \n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val = top_logits[:, -1] \n",
    "            logits = torch.where(\n",
    "                logits < min_val, \n",
    "                torch.tensor(float('-inf')).to(logits.device), \n",
    "                logits\n",
    "            )\n",
    "        if temperature > 0.0: \n",
    "            logits = logits /temperature \n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "        else:\n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you\"\". I it a,. his me \" my-- \"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "token_ids = generate(\n",
    "    model=model, \n",
    "    idx=text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens=15, \n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "    top_k=25, \n",
    "    temperature=1.4\n",
    ")\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"../models/model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.load_state_dict(torch.load(\"../models/model.pth\"))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    \"model_state_dict\": model.state_dict(), \n",
    "    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "    }, \n",
    "    \"model_and_optimizer.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load(\"model_and_optimizer.pth\")\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=0.1)\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('gpt_download.py', <http.client.HTTPMessage at 0x33111fa30>)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = (\n",
    "    \"https://raw.githubusercontent.com/rasbt/\"\n",
    "    \"LLMs-from-scratch/main/ch05/\"\n",
    "    \"01_main-chapter-code/gpt_download.py\"\n",
    ")\n",
    "filename = url.split('/')[-1]\n",
    "urllib.request.urlretrieve(url, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
